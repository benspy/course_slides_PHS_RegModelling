---
title: "Exercises day 2: Model building"

authors: 
  - name: "Guy-Alain Schnidrig (VPHI)"
  - name: "Filipe Miguel Maximiano Alves de Sousa (VPHI)"
  - name: "Eleftheria Michalopoulou (ISPM)"
  - name: "Beatriz Vidondo (VPHI)"
  - name: "Ben Spycher (ISPM)"

toc: true
toc-depth: 10
toc-location: left
number-sections: false
editor: source
format: 
  html:
    self-contained: true
    code-fold: true
    theme: flatly
---

### Information

These are the exercises for the [Linear and Logistic Regression in R](https://zuw.me/kurse/dt.php?kid=4476) course of the [Public Health Sciences Course Program](https://www.medizin.unibe.ch/studies/study_programs/phs_course_program) at the [University of Bern](https://www.unibe.ch/).

We will use the dataset “data_caerphilly_full.csv" to demonstrate the steps of the model “purposeful selection”.  

This dataset contains data from a cohort study in Whales on risk factors for cardiovascular disease.

The objective our analysis is to identify risk factors for myocardial infarction (outcome variable "mi").


```{r setup, include=FALSE, echo=FALSE, warning=FALSE, message=FALSE}
# Set language
Sys.setenv(LANG = "en")

# Clear memory
rm(list=ls())
gc()

# Load libraries
library_names <- c("tidyverse", "knitr", "broom", "gridExtra", "ggpubr", "ggpmisc", "generics", 
                   "performance", "qqplotr", "patchwork", "see", "MASS", "car", "pROC","faraway","epiDisplay","epiR","ggplot2","descr",
                   "ResourceSelection","flexmix")

lapply(library_names, function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x)
    library(x, character.only = TRUE)
  }
})
rm(library_names)

```

# Logistic regression

## Load data

Load "data_caerphilly_full.csv" data using the function "read_csv2()". 

```{r}
#| code-fold: false

# Load the csv file
dat<-read.csv2("../data/data_caerphilly_full.csv")
str(dat)
```

#### Exercise 1: variables check

Use `head()` to display the first 10 rows of the data.

```{r}
dat %>% 
  head(10) %>% 
  kable()
```

Look at the format of the different variables in the dataset. Transform those which are not in the correct format.\

Transform first to factors some of the variables that will be used ("mi","socclass", "diabetes", "smoking", "hbpsyst", "bmicat", "cursmoke"). Transform the 0/1 in the categorical variables with that format in "No" and "Yes", respectively.

Check that those which are numeric are in the numeric format. 

```{r}
# make factors out of character variables
dat<- dat %>% 
  mutate(mi = factor(mi, levels=c(0, 1), labels=c("No", "Yes")),
                    socclass = factor(socclass, levels=c("I", "II", "IIINM", "IIIM", "IV", "V")), 
                    diabetes=factor(diabetes, levels=c("No/uncertain", "Yes")), 
                    smoking=factor(smoking, levels=c("Never smoked", " Ex>5 years", "Ex 1-4 years" ,"<15 per day", ">15 per day")),
                    hbpsyst = factor(hbpsyst, levels=c(0, 1), labels=c("No", "Yes")),
                    hbpdias = factor(hbpdias, levels=c(0, 1), labels=c("No", "Yes")),
                    bmicat = factor(bmicat, levels=c("Underweight", "Normal", "Overweight", "Obese")),
                    cursmoke = factor(cursmoke, levels=c("No", "Yes")),
                    fibrin= as.numeric(fibrin),
                    totchol=as.numeric(totchol),
                    hdlchol=as.numeric(hdlchol),
                    bpdias=as.numeric(bpdias),
                    bmi=as.numeric(bmi),
                    bpsyst=as.numeric(bpsyst),
                    )

```

# Variable selection by steps

## Exercise 2: Variable selection

We always begin model building with an initial set of candidate predictors/risk factors which are selected based prior decisions and data availability. 

In addition to an ID variable and the outcome the dataset contains 13 variables. 

Based a priori considerations, we exclude 3 of these because they are categorizations of continuous variables that are included in the dataset: hpbsyt and hbpdias (high systolic and diastolic blood pressure) represent high values of the continuous variables bpsyst and bpdias respectively; and the bmicat (BMI categories) represents categories of the variable BMI. To avoid redundancy and loss of information, we only include the continuous versions of these variables. 

All the other variables include plausible risk factors. We will therefore consider "socclass", "diabetes", "cursmoke", "smoking", "fibrin", "totchol", "hdlchol", "bpsyst", "bpdias" and "bmi" for inclusion. 

It's often a good idea to have a vector of these variables for later use (e.g. to select on these variables or loop over them)

- Create first a vector containing the above listed variables "c()".

```{r}

vars<-c("socclass", "diabetes", "cursmoke", "smoking", "fibrin", "totchol", "hdlchol", "bpsyst", "bpdias", "bmi" )

```


## Step 1 - Univariable model regressions

Screen the pre-selected explanatory variables for inclusion one at a time using univariable logistic regressions with "mi" as the outcome variable. 

We show the process for three variables - "socclass", "smoking" and "fibrin" below. Check for the estimate values for plausibility (direction of association) and the Wald P-values. For each model calculate the Likelihood Ratio as well with "anova" command. We will include in the next step all variables with a P-value bellow 0.2. 

```{r}
#socclass  
model<-glm(mi ~ socclass  , data=dat, family=binomial(link="logit"))
summary(model)
anova(model, test="Chisq")[["Pr(>Chi)"]]

#smoking  
model<-glm(mi ~ smoking  , data=dat, family=binomial(link="logit"))
summary(model)
anova(model, test="Chisq")[["Pr(>Chi)"]]

#fibrin
model<-glm(mi ~ fibrin  , data=dat, family=binomial(link="logit"))
summary(model)
anova(model, test="Chisq")[["Pr(>Chi)"]]

```


To automate the process, we can use a "for loop" to go through all the variables to be screened and output the result in a summary table. 

```{r}

varname <- c()
pval <- c()
vars<-c("socclass", "diabetes",  "smoking", "cursmoke","fibrin", "totchol", "hdlchol", "bpsyst", "bpdias", "bmi" )

for (var in vars) {
  formula <- paste("mi ~", var )
  model<-glm(formula, data=dat, family=binomial(link="logit"))
  pval <- c(pval, anova(model, test="Chisq")[["Pr(>Chi)"]][2])
}
tab <- data.frame(vars,pval, pval<=0.20)
names(tab) <- c("Variable", "P-value LR-test", "Include (p<=0.2)")
kable(tab, digits=6)
```
### Questions: 

- Which variables would you include?


# Step 2: Full model - select for exclusion 

In this step we first include all the variables selected in step 1 and then suggest variable for exclusion based on a more stringen p-value (say 0.1)

Let's now build a multivariable model including all the variables selected during the first step.

```{r}

mod_f<-glm(mi ~ socclass + diabetes + cursmoke + smoking + fibrin + totchol + hdlchol + bpsyst + bpdias + bmi, data=dat, family=binomial(link="logit"))
summary(mod_f)


```

In case there is a warning message that the model did not converge we can increase the number of interactions. We can add "control=list(maxi=1000)" to the function "glm()". 

```{r}

mod_f<-glm(mi ~ socclass + diabetes + cursmoke + smoking + fibrin + totchol + hdlchol + bpsyst + bpdias + bmi, data=dat, family=binomial(link="logit"), control=list(maxi=1000))
summary(mod_f)

```

### Question:

The model leaves out the category “smoking>15 per day“. Why?

Change the order between "smoking" and "cursmoke" in the "glm()" function and check what happens. You can also tabulate the values of both variables with the function "table".  

```{r}

mod_f_changed<-glm(mi ~ socclass + diabetes  + smoking + cursmoke + fibrin + totchol + hdlchol + bpsyst + bpdias + bmi, data=dat, family=binomial(link="logit"))
summary(mod_f_changed) 

#and tabulate the values of the two variables
table(dat$smoking, dat$cursmoke)

```


### Question: 

- What is the difference? Why do you think it happens?


("cursmoke" = "Yes" includes all current smokers which, are made up of those with ">15" and "<15" cigarettes per day in the variable "smoking".\
The dummy variables for these three categories are perfectly collinear, that is each of them could be represented by a linear combination of the other two. One of them is thus redundant and must be dropped. This is what happened. The category that is dropped depends on the position of the dummy variable in the model). 


Equivalently, we can simply drop cursmoke. Note as we do this the global summary statistica such residual deviance and the AIC value remain the same. This is indeed an equivalent model 

```{r}

mod_f_alt<-glm(mi ~ socclass + diabetes  + smoking + fibrin + totchol + hdlchol + bpsyst + bpdias + bmi, data=dat, family=binomial(link="logit"))
summary(mod_f_alt)

```


We now want to again exclude any irrelevant variables (based on a P-value <0.1. 

### Question:
- Do the Wald tests allow you to make any firm conclusions for the categorical variables? Excplain


For the continuous variables, the Wald tests suggest to exclude "bmi" and "bpsyst".


### Question:
- Which variables would you like to test further?

### Exercise: 

Fit two restricted models, one without "socclass" and the other without "smoking".

```{r}

mod_r1<-glm(mi ~ diabetes  + smoking + fibrin + totchol + hdlchol + bpsyst + bpdias + bmi, data=dat, family=binomial(link="logit")) # without socclass
summary(mod_r1)

mod_r2<-glm(mi ~ socclass + diabetes  + fibrin + totchol + hdlchol + bpsyst + bpdias + bmi, data=dat, family=binomial(link="logit")) # without smoking
summary(mod_r2)


```
Let us now compare the two new restricted models with the full model "mod_f". 
Run a LR-test for the restricted model against the full model "mod_f".


```{r}

anova(mod_r1, mod_f_alt, test="Chisq") 
anova(mod_r2, mod_f_alt, test="Chisq")  

```

### Questions: 

- How do you interpret the results of each comparison?

- What would be your decision on the inclusion/exclusion of the variables?

- Which variables would you include?





## New model

Based on the previous results, we can exclude "bmi", "bpsyst", "socclass" and "cursmoke".

Build a new reduced model, excluding the referred variables, and compare it with the model including the referred variables. 

```{r}

mod_r<-glm(mi ~ diabetes + smoking + fibrin + totchol + hdlchol +  bpdias , data=dat, family=binomial(link="logit"))
anova(mod_r,  mod_f_alt, test="Chisq") 

```

Look also at the AIC values of the two models, using the function "summary()" and at the BIC values using the function "BIC()".

```{r}
glance(mod_f_alt)
glance(mod_r)

# Or
AIC(mod_f_alt)
AIC(mod_r)
BIC(mod_f_alt)
BIC(mod_r)
```


### Questions: 

- Looking at the comparison between both models, which model would you favor? 



## Exercise: checking for confounding by excluded variables

Another point we should consider when excluding these variables is the possibility that associations of remaining variables with the outcome are now confounded because by the excluded variables. 

Let's compare the estimates again between the two models 
```{r}
tidy(mod_f_alt)
tidy(mod_r)
```



- Looking at the coefficient of included variables, do you think there might exist any confounding? 

- For which ones? 


```{r}

mod_test1<-glm(mi ~ diabetes + smoking + fibrin + totchol + hdlchol +  bpdias , data=dat, family=binomial(link="logit"))

mod_test2<-glm(mi ~ diabetes + smoking + fibrin + totchol + hdlchol+ bpsyst+ bpdias , data=dat, family=binomial(link="logit"))


summary(mod_test1)
summary(mod_test2)
```

The only coefficient that has a major change is "bpdias", but this is understandable as diastolic blood pressure are correlated. 

"bpdias" appears to be adequately able to replace the little explanatory information that "bpsyst" was adding



# Step 4: Reconsider initially excluded variables (step 1) for re-inclusion

We did not exclude any variables at step 1, so nothing to do for this step. 



# Step 5: Assess the appropriate functional form for the continuous variables

Let us now verify the assumption of linear associations (on the logit scale) for continuous variables.

We can start with the variable "fibrin". 

## Method 1 for assessing linearity in logits

First step: Create a factor variable assigning individuals to quintiles of the continuous variable

```{r}

# define a factor variable representing quintils of fibrin
quint <- quantile(dat$fibrin, seq(0,1,0.2))
dat$fibrin_q <- cut(dat$fibrin, quint, include.lowest = TRUE)

```

Second step: Create a numeric variable assigning individuals to the median/midpoint value within their category

```{r}
# a numeric variable representing medians within quintiles
qmed <- tapply(dat$fibrin, dat$fibrin_q, median)
dat$fibrin_qm <- qmed[dat$fibrin_q]
```

Third step: Fit the regression model once with the factor variable and once with the numeric variable entered as linear term.

```{r}
# run regressions
temp1<-glm(mi ~ diabetes + smoking + fibrin_qm+ totchol + hdlchol +  bpdias , data=dat, family=binomial(link="logit"))
temp2<-glm(mi ~ diabetes + smoking + fibrin_q + totchol + hdlchol +  bpdias , data=dat, family=binomial(link="logit"))

```

Fourth step: Plot the the logits of both models (keeping values of other covariates fixed) against the medians/midpoints of the categories

```{r}
# Create the data to plot
new <- dat[1,] |> dplyr::select(diabetes, smoking, totchol,hdlchol, bpdias) # Takes out 1 covariate pattern which we will keep constant
new <- cbind(new, fibrin_q=levels(dat$fibrin_q), fibrin_qm=qmed) # add the quintile variables
new$fit1<-predict(temp1, newdata=new) # add the predicted logit 
new$fit2<-predict(temp2, newdata=new) # add the predicted logit 

# Plots logit scale
ggplot(data=new, aes(x=fibrin_qm)) +
  geom_vline(xintercept = quint, linetype="dotted",  color = "blue") +
  geom_line(aes(y=fit1), color="red") +
  geom_line(aes(y=fit2), col="blue") +
  xlab("fibrin") + ylab("logit") +
    geom_text(data = NULL, x = 5, y = -1.55, label = "Linear on quintile medians", color="red", size = 5, hjust="left" ) +
  geom_text(data = NULL, x = 5, y = -1.45, label = "Unrestricted on quintiles", color="blue", size = 5, hjust="left") +
  theme_minimal() +
  theme(text = element_text(size = 20))

```
### Question: 
- Based on this method, would you assume linearity? Explain.

## Method 2 for assessing linearity in logits

- Fit the the model including the continuous variable as a linear term, i.e. assuming linearity in the logit.
- Predict the probability of the outcome for each individual based on the fitted model. 
- Plot a 'locally estimated scatterplot smoothing' (LOESS) function of these predicted probabilities over the range of the continuous variable.  
- In the same plot, add a LOESS function of the outcome, optionally with pointwise confidence intervals. 
- Agreement between the plots affirms linearity in the logit.

```{r}
# 1
temp<-glm(mi ~ diabetes + smoking + fibrin + totchol + hdlchol +  bpdias , data=dat, family=binomial(link="logit"))
# 2
dat$fit <- temp$fitted.values
# 3, 4
dat$mi_n <- as.numeric(dat$mi=="Yes")
ggplot(data=dat, aes(x=fibrin, y=mi_n))+
  geom_point(alpha = 1/10) + 
  geom_smooth(method="loess", se=TRUE) + 
#  geom_line(data=new, aes(y=fit, x=fibrin), color="red") +
  geom_smooth(aes(y=fit), method="loess",  color="red", se=FALSE) +
  xlab("fibrin") + ylab("probability") +
  geom_text(data = NULL, x = 1, y = .7, label = "LOESS of predictions based on linear logits", color="red", size = 5, hjust="left" ) +
  geom_text(data = NULL, x = 1, y = .8, label = "LOESS of outcome", color="blue",  size = 5, hjust="left") +
    theme_minimal() +
  theme(text = element_text(size = 20)) 
```

### Question: 
- Based on this method, would you assume linearity? Explain.


# Step 6: Check for interactions

We should now assess plausible interactions between explanatory variables. A possible one is an interaction between total cholesterol ("totchol") and "smoking". 

### Exercise: check for interaction between "totchol" and "smoking".

```{r}
mod_r_int<-glm(mi ~ diabetes + smoking * totchol + fibrin + hdlchol +  bpdias , data=dat, family=binomial(link="logit"))
summary(mod_r_int)
anova(mod_r, mod_r_int, test="Chisq") # -> exclude interaction

```

### Questions: 

- What does this mean?

- Would you include this interaction in the final model?


### Model interpretation

Look at the summary of the final model

```{r}
summary(mod_r)


```

### Questions: 

- How do you interpret the estimate value for "fibrin"?

- How would it be easier to interpret the results of the model?

- Can you do it for smoking Ex>5 years ? Can you calculate the CI? What does it mean?

```{r}
cbind(exp(coef(mod_r)[3]), exp(confint.default(mod_r, 'smoking Ex>5 years', level=0.95)))
```

- And for "hdlchol"? What does it mean?

```{r}

cbind(exp(coef(mod_r)["hdlchol"]), exp(confint.default(mod_r, 'hdlchol', level=0.95)))

```

- Calculate the OR for all the variables

```{r}

cbind(exp(coef(mod_r)),  exp(confint.default(mod_r)))

```
- Which variables can be considered risk factors?

### Exercise: Prediction

Using the function "predict()", calculate the predicted risk for mi of someone with "diabetes"="Yes", "smoking"="<15 per day", "fibrin"=3, "totchol"=5, "hdlchol"=0.1 and "bpdias"=8

```{r}

new <- data.frame(diabetes="Yes", smoking="<15 per day", fibrin=3, totchol=5,hdlchol=0.1,bpdias=8)

prediction<-predict(mod_r, newdata=new, type = "response")
prediction
```



---
title: "Introduction to logistic regression"
author: "Ben Spycher"
format: ctupres-revealjs
# footer: |
#         CTU Bern theme for [Quarto Presentations](https://quarto.org/docs/presentations/revealjs/index.html).
#         Code available on [GitHub](https://github.com/aghaynes/CTUquarto).
chalkboard: false
self-contained: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(knitr)
library(RColorBrewer)
library(epitools)
library(ggplot2)
library(kableExtra)
```

## Program overview

| Day     | Time   | Topic                                                  |
|---------|--------|--------------------------------------------------------|
| Mon     | AM     | Simple linear regression                               |
|         | PM     | Multiple linear regression                             |
| **Tue** | **AM** | **Introduction to logistic regression**                |
|         | PM     | Model building considerations and strategies           |
| Wed     | AM     | Models for stratified designs and categorical outcomes |
|         | PM     | Exercises, QA, wrap-up                                 |

## This mornings's topics

|                                                       |
|-------------------------------------------------------|
| **Introduction to logistic regression**
|	Generalizing the linear model (link functions, maximum likelihood estimation)
|	The logistic model (logistic link, binomial distribution)
|	Interpreting of coefficients (logits, odds ratios)
|	Interactions


## Data types

![[Image by Siva Sivarajah](https://towardsdatascience.com/statistical-testing-understanding-how-to-select-the-best-test-for-your-data-52141c305168)](pics/data_types.jpg){width="90%" fig-align="left"}

## Common regression models

| **Scale**   | ***Examples***           | **Typical models**       |
|-------------|--------------------------|--------------------------|
| Continuous  | *BMI, blood pressure*    | Linear regression        |
| Dichotomous | *Disease/death (yes/no)* | Logistic regression      |
| Nominal     | *3 disease subtypes*     | Multinomial logist. reg. |
| Ordinal     | *4 severity stages*      | Ordered logist. reg.     |
| Count       | *Number of cases*        | Poisson regression       |


## What is a statistical model



## Parameters of interest
* The plasma volumes of 8 males were 2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12 litres. We estimate the mean ($\mu$) and standard deviation ($\sigma$).  
* Nine of 12 household contacts of a SARS-CoV-2 positive index case also tested positive; the other 3 tested negative. We estimate $\pi$, the probability of transmission.  
* We estimate the odds ratio for the association between coronary heart disease and high systolic blood pressure in a case control study.  


## More formal definition
* A *statistical model* defines a family of probability distributions from which the observed data are hypothesized to arise from.

* The individual distributions within the family are "indexed" (identified) by a set of *parameters*.

* The purpose of estimation is to identify the "true" distribution, i.e. the "true" parameter values (*point estimation*), or a plausible range of the parameter values (*interval estimation*).

## Example linear model

A simple linear regression model can be specified as
$$Y_i=\beta_0+\beta_1x_i + \varepsilon_i$$
where the $\varepsilon_i$ ($i=1,2,...,n$) are independent and normally distributed with mean 0 and variance $\sigma^2$. 

The parameters of the model are $\beta_0$, $\beta_1$, and  $\sigma^2$.

Specific values of these parameter uniquely identify a (conditional) distribution of $Y_i$ (given $x_i$).

## Example binomial distribution

Nine of 12 household contacts of a SARS-CoV-2 positive index case also tested positive; the other 3 tested negative.   

Let's model the number of positive tests $Y$ as a binomial distribution with parameters $\pi$ (the unknown probability of a positive test) and $n=12$ (known):

$$P_{n,\pi}(Y=y)=\binom{n}{y}\pi^y(1-\pi)^{n-y}$$

## Maximum likelihood estimation

In ML estimation we estimate parameters by the values that maximise the 'likelihood' of the observed data. 

Formally, we maximize the *likelihood function*, which in our case is

$$\mathcal{L}(\pi|y=9,n=12)=P_{n=12,\pi}(Y=9)=\binom{12}{9} \pi^9(1-\pi)^{3}$$
*Note*: the likelihood function is a function of the parameter of interest given the observed data.

## Maximum likelihood estimation


```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
L<-function(p){
  choose(12,9)*p^9*(1-p)^(3)
  }
p<-seq(0,1,0.001)
Likelihood<-L(p)
par(cex=1.3, mai=c(1,1,.5,.5))
plot(p,Likelihood, type="l", col="green", xlab=expression(paste(pi)), lwd=2)
```

## Maximum likelihood estimation

The ML estimator turns out to be the relative frequency $\hat{\pi}_{ML}=\frac{k}{n}$ (our standard estimator for a probability).
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
L<-function(p){
  choose(12,9)*p^9*(1-p)^(3)
  }
p<-seq(0,1,0.001)
Likelihood<-L(p)
par(cex=1.3, mai=c(1,1,.5,.5))
plot(p,Likelihood, type="l", col="green", xlab=expression(paste(pi)), lwd=2)
abline(v=9/12, lty=3, col="blue", lwd=2)
text(0.42, 0.021, expression(hat(pi)), col="blue")
text(0.60, 0.02, "=9/12=0.75",  col="blue")
```

## Generalizing the linear model

In linear regression the conditional mean of an outcome $Y\in \mathbb{R}$ given a value $x$ of a covariate $X$ as 
$$
\mathrm{E}(Y|X=x)=\beta_0+\beta_1x
$$
with $Y$ distributed normally around this mean.

This means that, as $x$ varies between $-\infty$ and $\infty$, $\mathrm{E}(Y|X=x)$ can also take on any values between $-\infty$ and $\infty$.

## Generalizing the linear model
However, many outcomes have a mean that is limited to a specific range and follow a non-normal distribution. 

Examples: 

* Dichotomous outcome: 
  * $Y\in \{0,1\}$ and $\mathrm{E}(Y)\in[0,1]$ 
  * $P(Y=1)=\pi$, $P(Y=0)=1-\pi$ (Bernouilli distribution)
  
* Count outcome: 
  * $Y \in\{0,1,2,...\}$ and $\mathrm{E}(Y)\in(0,\infty)$ 
  * $Y$ commonly modelled using Poisson distribution
  
## Generalizing the linear model
We can generalise the linear model as follows: 
$$
g(\mathrm{E}(Y|X=x))=\beta_0+\beta_1x
$$
where $g$ is a 1:1-function, the so-called *link function*, that maps the range of $\mathrm{E}(Y)$ to the interval $(-\infty,\infty)$. 

Equivalently, we can write: 

$$
\mathrm{E}(Y|X=x)=g^{-1}(\beta_0+\beta_1x)
$$
Where $g^{-1}$ is the inverse function of $g$.

## Modelling a dichotomous outcome
If we code a dichotomous outcome as $Y=1$ if the event occurs and $Y=0$ if it doesn't, the probability distribution of $Y$ is then given by a *Bernouilli distribution*: 

$$P(Y=y) =\begin{cases} \pi \text{ , if } y=1 \\ 1-\pi \text{ , if } y=0 \end{cases} $$
We can rewrite this as (small exercise): 

$$ P(Y=y) =\pi^y (1-\pi)^{1-y}  $$ 

## Modelling a dichotomous outcome
The parameter $\pi$, which is the probability that $Y=1$, is, at the same time, the expected value of $Y$:

$$E(Y)=\pi= P(Y=1)$$

Therefore, we are looking for a suitable link function $g$ to connect $\pi$ (taking on values between 0 and 1) with the linear term $\beta_0+\beta_1x$ (taking on values between $-\infty$ and $\infty$):

$$
\mathrm{E}(Y|X=x)=P(Y=1|X=x)=\pi(x)=g^{-1}(\beta_0+\beta_1x)
$$


## Example: coronary heart disease
Dataset containing age (X) and presence ($Y=1$) or absence ($Y=0$) of coronary heart disease (CHD) for n=100 individuals:
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
dat<-read.csv("data/chdage.csv")
par(cex=1.3, mai=c(1,1,.5,.5))
plot(dat$age, dat$chd, xlab="X (Age)", ylab="Y (CHD)" , yaxp=c(0,1,1))
age_f<-cut(dat$age,seq(0,99,10), right=FALSE)
dat_agg<-aggregate(dat,list(age_f,age_f),mean)


```


## Example: coronary heart disease
Let's add the relative frequency (as probability estimate) of Y by 10 year age groups. The S-shape resembles a cumulative distribution function (CDF). 
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
par(cex=1.3, mai=c(1,1,.5,.5))
plot(dat$age, dat$chd, xlab="X (Age)", ylab="Relative frequency CHD",cex.axis=0.8 )
age_f<-cut(dat$age,seq(0,99,10), right=FALSE)
dat_agg<-aggregate(dat,list(age_f,age_f),mean)

abline(v=seq(0,99,10), col="red",lty=3)
points(dat_agg$age, dat_agg$chd, col="red")
lines(dat_agg$age, dat_agg$chd, col="red")

```



## CDF of the standard normal
We could e.g. use the CDF of a standard normal distribution to model the probabilities:
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
x<-seq(-3,3,0.01)
pdf<-dnorm(x)
cdf<-pnorm(x)
par(cex=1.3, mai=c(1,1,.5,.5))
plot(x,pdf, type="l", xlab="z", ylab="CDF standard normal", col="grey",lty=3, ylim=c(0,1), cex.axis=0.8 )
lines(x,cdf, col="red")

```


## CDF of the logistic distribution
A common choice is the CDF of the standard logistic distribution:

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
x<-seq(-3,3,0.01)
pdf<-dlogis(x, 0,sqrt(3/pi^(2)))
cdf<-plogis(x, 0,sqrt(3/pi^(2)))
par(cex=1.3, mai=c(1,1,.5,.5))
plot(x,pdf, type="l", xlab="z", ylab="CDF Logistic distribution", col="grey",lty=3, ylim=c(0,1), cex.axis=0.8 )
lines(x,cdf, col="red")

```

## Regression models for dichotomous outcomes
Two commonly used regression models are: 

* Probit regression: 
  * CDF of standard normal used as inverse link function $g^{-1}$
  * Frequently used in economics
  
* Logistic regression: 
  * CDF of logistic distribution used as inverse link function $g^{-1}$
  * Common choice in biostatistics/epidemiology

## Comparison of logistic and standard normal distributions
 
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
x<-seq(-3,3,0.01)
pdfl<-dlogis(x, 0,sqrt(3/pi^(2)))
cdfl<-plogis(x, 0,sqrt(3/pi^(2)))
pdfn<-dnorm(x)
cdfn<-pnorm(x)
par(cex=1.3, mai=c(1,1,.5,.5))
plot(x,pdfl, type="l", xlab="z", ylab="CDF Logistic distribution", col="blue",lty=3, ylim=c(0,1), cex.axis=0.8 )
lines(x,cdfl, col="blue")
lines(x,pdfn, col="red")
lines(x,cdfn, col="red")
text(-3,0.9,"Standard normal", col="red", pos=4)
text(-3,0.8,"Logistic with mean=0, SD=1", col="blue", pos=4)

```



## Logistic regression

The *logistic function* (CDF of the logistic distribution) is given by $g^{-1}(z)=\frac{e^z}{1+e^z}$. 

Replacing $z$ with our linear term $\beta_0+\beta_1x$, we model the conditional probability of $Y=1$ given $X=x$ as

$$
\mathrm{E}(Y|X=x)=P(Y=1|X=x)=\pi(x)=\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}
$$
Note that the probability of $Y=0$ given $X=x$ is
$$
1-\pi(x)=\frac{1}{1+e^{\beta_0+\beta_1x}}
$$

## Logistic regression

The corresponding link function is called the *logit function* $g(\pi)=\log\left(\frac{\pi}{1-\pi}\right)$

Thus, our model can be equivalently written as: 

$$g(\pi(x))=\log\left(\frac{\pi(x)}{1-\pi(x)}\right)=\beta_0+\beta_1x$$
The linear term $\beta_0+\beta_1x$, in this context, is often simply referred to as *logit*. It has the convenient interpretation as the log odds of the outcome. 


## Generalised linear models
In R, the `glm` command is used to fit logistic regression models.

*Generalised linear models (GLMs)* provide a common framework for modelling outcomes of different scale (continuous, count, categorical) and are specified as:  

* $g(\mathrm{E}(Y|X=x))=\beta_0+\beta_1x$ with given *link function* $g$
* The distribution of $Y$ is a member the *exponential family*  

The *exponential family* includes the normal, Bernouilli, binomial, Poisson and other distributions.



## The logistic model as a GLM
The logit link function arises naturally when we rewrite the Bernouilli distribution in it's exponential form: 

$$
\begin{aligned}
P(Y=y) &=\exp\{\log\{\pi^y (1-\pi)^{1-y}\}\}  \\ 
  &= e^{y\log (\pi)+(1-y)\log (1-\pi)} \\ 
  &= e^{y \left(\log (\pi)-log (1-\pi)\right)+\log (1-\pi)} \\
  &= e^{y \cdot \log \left(  \frac{\pi}{1-\pi} \right)+\log (1-\pi)} 
\end{aligned} 
$$
$P(Y=y)$ depends on $y$ only through the product term in the exponent. The factor with which $y$ is multiplied is easily recognized as the logit of $\pi$.



## Fitting the model
The parameters $\beta_0$ and $\beta_1$ are estimated by ML estimation. 

Fitting logistic regression model for the risk of CHD in R: 

::: {style="font-size: 80%;"}
```{r, echo=TRUE}
mod<-glm(chd~age, data=dat, family=binomial(link="logit"))
summary(mod)
```
:::


## The regression equation
On the **logit scale** the fitted model equation is: 

$$\text{logit}(\pi(\text{age}))=\log\left(\frac{\pi(\text{age})}{1-\pi(\text{age})}\right)= -5.30945 + 0.11092 \cdot \text{age}$$

Conversely, on the **probability scale** we have: 
$$\pi(\text{age})=\frac{e^{\text{logit}(\pi(\text{age}))}}{1+e^{\text{logit}(\pi(\text{age}))}} =\frac{e^{-5.30945 + 0.11092 \cdot \text{age}}}{1+e^{-5.30945 + 0.11092 \cdot \text{age}}}$$

## Visualisation on the probability scale
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
par(cex=1.3, mai=c(1.2,1,.5,.5))
plot(dat$age, dat$chd, xlab="X (Age)", ylab="Predicted probability CHD",cex.axis=0.8 , xlim=c(20,70))
age_f<-cut(dat$age,seq(0,99,10), right=FALSE)
dat_agg<-aggregate(dat,list(age_f,age_f),mean)

abline(v=seq(0,99,10), col="red",lty=3)
points(dat_agg$age, dat_agg$chd, col="red")
lines(dat_agg$age, dat_agg$chd, col="red")
newdat<-data.frame(age=seq(0,99,1))
newdat$pred<-predict.glm(mod,newdata = newdat,type="response")
lines(newdat$age, newdat$pred, col="blue")
text(20,0.8,"Based on logistic regression",pos=4, col="blue")
text(20,0.7,"Based on relative frequency within age groups",pos=4, col="red")
```



## Visualisation on the logit scale
 
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
par(cex=1.3, mai=c(1.2,1,.5,.5))
age_f<-cut(dat$age,seq(0,99,10), right=FALSE)
dat_agg<-aggregate(dat,list(age_f,age_f),mean)

logit<-function(pi){log(pi/(1-pi))}

newdat<-data.frame(age=seq(0,99,1))
newdat$pred<-predict.glm(mod,newdata = newdat,type="response")
plot(newdat$age, logit(newdat$pred), xlab="X (Age)", ylab="Predicted logit", xlim=c(20,70), type="l", col="blue" )
abline(v=seq(0,99,10), col="red",lty=3)
points(dat_agg$age, logit(dat_agg$chd), col="red")
lines(dat_agg$age, logit(dat_agg$chd), col="red")
text(20,4,"Based on logistic regression",pos=4, col="blue")
text(20,3,"Based on relative frequency within age groups",pos=4, col="red")
```





## Predicting odds and probabilities
```{r}
#| echo: false
#| eval: false
newdat<-data.frame(age=60)
newdat$logit<-predict.glm(mod,newdata = newdat)
newdat$odds<-exp(newdat$logit)
newdat$prob<-predict.glm(mod,newdata = newdat,type="response")
newdat

```

For a person aged 60 years the logit is: 

$$\text{logit}= -5.30945 + 0.11092 \cdot 60 = 1.345815$$

The predicted odds for CHD is thus: 

$$\text{odds}=e^{1.345815}=3.841317$$
The predicted probability (risk in this context) for CHD is: 

$$\pi=\frac{3.841317}{1+1.345815}=0.793445$$



## Interpreting the intercept $\hat{\beta_0}$
The intercept $\hat{\beta_0}$ is the estimated *log odds* of the outcome when the independent variables are set to 0.

The practical interpretation depends on meaning of zero: 

* Present case:  $e^{\hat{\beta_0}}=$ estimated  odds for age 0 years (no practical meaning)
* $x$ is a centered continuous variable: $e^{\hat{\beta_0}}=$ estimated odds when $x$ is set to the sample mean
* $x$ is a dichotomous variable (0,1 coding): $e^{\hat{\beta_0}}=$ estimated odds for reference category (baseline log odds)


## Interpreting the slope $\hat{\beta_1}$
The intercept $\hat{\beta_0}$ estimates the change in log odds when the independent variable $x$ increased by 1 unit: 

For any given value of $x$:
$$\log(\text{odds}(x+1))-\log(\text{odds}(x)) = \hat{\beta_0}+\hat{\beta_1}(x+1) - \hat{\beta_0} - \hat{\beta_1}x = \hat{\beta_1}$$
Thus $\hat{\beta_0}$ represents the *log odds ratio* (log OR) corresponding to a unit change in x:   

$$\hat{\beta_1}= \log \left(\frac{\text{odds}(x+1)}{\text{odds}(x)} \right)$$

## Interpreting the slope $\hat{\beta_1}$
Therefore $e^{\hat{\beta_0}}$ is the estimated OR corresponding $x$ increased by 1 unit in $x$. 

The practical intepretation depends on the meaning of a unit change: 

* Our example: $e^{0.11092}= 1.1173$ is the estimated OR for CHD corresponding to an increase in age by 1 year.
* $x$ is a dichotomous exposure variable (0,1 coding): $e^{\hat{\beta_0}}$ is the estimated OR for the outcome comparing exposed to non-exposed individuals. 

## Differences in logits
Differences in logits correspond to log ORs. We can thus extract ORs for more general comparisons from the regression equation. 

Example:  OR for CHD comparing persons aged 60 with persons aged 40 years:

$$\log(\text{odds}(60))-\log(\text{odds}(40)) = \hat{\beta_0}+\hat{\beta_1} 60 - \hat{\beta_0} - \hat{\beta_1}40 = 20 \cdot \hat{\beta_1}$$
$$\text{OR} = e^{20 \cdot \hat{\beta_1}}= e^{20 \cdot 0.11092}= 9.19261$$


## Multiple logistic regression

The logistic regression model can be extended to include multiple independent variables.   

On the logit scale, we write the regression equation: 
$$\text{logit}(\pi(x_1, x_2,\dots,x_k))=\beta_0 + \beta_1 x_1 +\beta_2 x_2 +\dots +\beta_k x_k$$

On the probability scale the model becomes: 
$$
\pi(x_1, x_2,\dots,x_k)=\frac{e^{\beta_0 + \beta_1 x_1 +\beta_2 x_2 +\dots +\beta_k x_k}}{1+e^{\beta_0 + \beta_1 x_1 +\beta_2 x_2 +\dots +\beta_k x_k}}
$$

##	Onchocerciasis dataset

Data on $n=$ 1032 individuals from a study of Onchocerciasis (river blindness) in Sierra Leone ([McMahon et al., Trans
Roy Soc Trop Med Hyg 1988; 82:595–600](https://doi.org/10.1016/0035-9203(88)90524-X)). 

Variables of interest: 

* `mf`: infection with microfilariae of *Onchocerciasis volvulus* (Yes/No)
* `area`: Area of village (savannah or rainforest) 
* `agegrp`: Age group (5–9, 10–19, 20–39, 40+)


##	Onchocerciasis dataset

Data from a random sample of 10 study members: 

::: {style="font-size: 80%;"}
```{r}
#| echo: false
#| eval: true
dat<-read.csv("data/onch1302.csv", sep=";")

dat<- dat |> select(area, agegrp, mf) |> mutate(mf = factor(mf, levels=c(0, 1), labels=c("No", "Yes")),
                    agegrp = factor(agegrp, levels=c(0, 1, 2, 3), labels=c("5–9", "10–19", "20–39", "40+")), 
                    area = factor(area, levels=c(0, 1), labels=c("Savannah", "Rainforest")))

dat$area<-relevel(dat$area, "Savannah")
set.seed(1234)
kable(dat[sample(1:dim(dat)[1],10), ])

```
:::

##	Dichotomous independent variable
Prevalence of infection by area: 

![ ](pics/oncho_area_mf.png){width=90%;fig-align="center"}

OR for infection comparing exposed to unexposed: 

$$\text{OR} = \frac{541/213}{281/267}=\frac{2.540}{1.052}=2.413$$

##	Dichotomous independent variable
Prevalence of infection by area: 

![ ](pics/oncho_area_mf.png){width=90%}

OR for infection comparing exposed to unexposed: 

$$\text{OR} = \frac{541/213}{281/267}=\frac{2.540}{1.052}=2.413$$

##	Dichotomous independent variable
Rewriting the odds in terms of the baseline odds and the OR: 

On the odds scale: 
![ ](pics/oncho_dich_odds.png){width=90%}


On the log odds scale: 
![ ](pics/oncho_dich_logodds.png){width=90%}

##	Dichotomous independent variable
Let's fit a logistic regression: 

::: {style="font-size: 80%;"}
```{r, echo=TRUE}
mod<-glm(mf~area, data=dat, family=binomial(link="logit"))
summary(mod)
```

:::

##	Dichotomous independent variable
Estimated parameters from logistic regression (logit scale): 

* Baseline log odds: $\hat{\beta_0}=0.05111$
* Log OR: $\hat{\beta_1}=0.88102$

.. on the odds scale: 

* Baseline odds: $e^{\hat{\beta_0}}=1.05243$
* OR: $e^{\hat{\beta_1}} = 2.41336$

##	Dichotomous independent variable



::: {style="font-size: 90%;"}

|       | Log odds    |
|:---------------------------------|:-------------|
| Exposed ($x=1$)    | $\hat{\beta_0}+1\cdot\hat{\beta_1}=0.0511+0.8810=0.9321$ |
| Unexposed ($x=0$)  | $\hat{\beta_0}+0\cdot\hat{\beta_1}=0.05111$ |

:::



::: {style="font-size: 90%;"}

|       | Odds    |
|:---------------|:---------------|
| Exposed ($x=1$)    | $e^{\hat{\beta_0}}\cdot e^{\hat{\beta_1}}=1.0524\cdot2.4134=2.5399$ |
| Unexposed ($x=0$)  | $e^{\hat{\beta_0}}=1.0524$ |

:::


##	Categorical independent variable

Microfilarial infection by age group: 

![Source: Kirkwood BR, Sterne JAC, Essential Medical Statistics, 2nd ed. Table 19.9, p. 199](pics/oncho_agegrp_mf.png){fig-align="center"}


##	Categorical independent variable
Rewriting the odds in terms of the baseline odds and the ORs: 

![Source: Kirkwood BR, Sterne JAC, Essential Medical Statistics, 2nd ed. Table 19.10, p. 199](pics/oncho_cat_odds.png){fig-align="center"}

Here "Baseline" refers to the baseline odds and Agegrp(1), Agegrp(2), and Agegrp(3) are ORs comparing the corresponding age groups with the reference group (5-9 years) 

##	Categorical independent variable
Again, we can also estimate these parameters using logistic regression. We use the following dummy coding for age group

|agegrp |$x_1$ | $x_2$| $x_3$|
|-----|-----|-----|-----|
|5-9 |0|0|0|
|10-19 |1|0|0|
|20-29 |0|1|0|
|40+ |0|0|1|

... and fit the regression equation: 
$$\text{logit}(\pi(x_1, x_2,x_3))=\beta_0 + \beta_1 x_1 +\beta_2 x_2 +\beta_3 x_3$$


##	Categorical independent variable
Fitted model: 

::: {style="font-size: 80%;"}
```{r, echo=TRUE}
mod<-glm(mf~agegrp, data=dat, family=binomial(link="logit"))
summary(mod)
```
:::

##	Categorical independent variable
It is easy to verify that we obtain the same baseline odds and ORs: 

```{r}
#| echo: true
#| eval: true
cbind(Estimates=mod$coefficients, Exponentiated=exp(mod$coefficients))
```

In simple logistic regression with a single categorical independent variable, maximum likelihood estimation yields the same baseline odds and ORs as we would calculate manually. 


##	Adjusting for confounding

Tabulating `agegrp` against `area`

```{r}
#| echo: true
#| eval: true
tbl0<-table(dat$agegrp, dat$area)
tbl<-cbind(tbl0, round(100*proportions(tbl0, margin=2),1))
colnames(tbl) <- c("Savannah (n)", "Rainforest(n)", "Savannah (col %)", "Rainforest (col %)")
tbl
```

... we see differences in the age distribution between savannah and rainforest areas, suggesting that the observed univariable associations might be confounded. 

##	Adjusting for confounding
![ ](pics/dag_mf.png){fig-align="center"}

## Adjusting for confounding
* Confounding: The observed association between an outcome and an independent is (at least partially) due to shared a common cause and is thus spurious.

* For a thorough discussion in the context of causal inference see [Hernán MA, Robins JM (2020). Causal Inference: What If. Boca
Raton: Chapman & Hall/CRC](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)

* Approaches to adjust for confounding include: 
  - Standardization 
  - Stratification
  - Multiple regression analysis


## Adjusting for confounding
Adjusting for confounding in regression analysis: 

* Variables representing potential confounders are included in addition to independent variable of interest (e.g. exposure/treatment)

* A relevant change in the effect measure (e.g. OR) indicates confounding 

* A common criterion used in epidemiology requires relative change of at least 10%

## Model with two variables

::: {style="font-size: 90%;"}
```{r}
#| echo: true
#| eval: true
mod<-glm(mf~area + agegrp, data=dat, family=binomial(link="logit"))
summary(mod)
```
:::

## Adjusting for confounding
* Crude OR comparing rainforest with savannah areas: $2.413$
* OR adjusted for age group (current model):  $e^{1.1260} = 3.083$

This difference suggests confounding by age group. 

Note that the ORs for age groups have also changed somewhat: 
* Crude OR comparing age group 40+ to 5-9 years: $16.024$
* OR adjusted for area (current model):  $e^{2.8703} =  17.642$


## Interpreting adjusted ORs
Rainforest versus Savannah:

* Persons of a given age group are estimated to have higher odds of microfilarial infection by a factor of 3.1 if they live in rainforest areas compared to savannah areas.

Age group 40+ to 5-9 years:

* In a given area (Savannah or rainforest), persons aged 40+ are estimated to have higher odds of microfilarial infection by a factor of 17.6 compared to children aged 5-9 years.

## Homogeneity assumption
The model implicitly assumes homogeneity of effects: 

* The ORs comparing rainforest with savannah constant accross age groups
* The ORs comparing age groups are the same in savannah and rainforest areas

## Homogeneity assumption
![Source: Kirkwood BR, Sterne JAC, Essential Medical Statistics, 2nd ed. Table 22.2b, p. 207](pics/oncho_adjusted_odds.png){fig-align="center"}

## Homogeneity assumption
These assumptions imply parallel shifts on the logit scale

![Source: Kirkwood BR, Sterne JAC, Essential Medical Statistics, 2nd ed. Fig 20.1, p. 210](pics/oncho_parallel.png){fig-size=80%; fig-align="center"}

## Effect modification
We can relax the homogeneity assumption and allow the association between one variable, A, and the outcome to vary across levels of another variable, B. 

If indeed it varies, we speck of *effect modification* (strictly speaking this terminology should be reserved to causal associations).

B is said to: 

*  "modify the effect" of A or ..
*  be an "effect modifier" of the effect of A on the outcome.

In regression analysis we assess effect modification by including interaction terms.

## Interaction terms

Using the formula `mf ~ area*agegrp` we can include *interaction terms* representing the specific effects of age groups in rainforest areas only:

::: {style="font-size: 75%;"}
```{r}
#| echo: false
#| eval: true
dat2<- dat |> select(mf,area, agegrp) |> 
  mutate(agegrp = factor(agegrp, levels=c("5–9", "10–19", "20–39", "40+"), labels=c("0","1","2","3")), 
         area = factor(area, levels=c("Savannah", "Rainforest"), labels=c("0", "1"))) |> 
  rename(A = area, B = agegrp)
  
x<-kable(unique(cbind(dat[,c("area","agegrp")], model.matrix(mf~A * B, data=dat2))), row.names=FALSE)
add_header_above(x,c("","","b_0","b_1" , "b_2", "b_3", "b_4", "b_5", "b_6", "b_7"))
```
:::

## Model with interaction

::: {style="font-size: 90%;"}
```{r}
#| echo: true
#| eval: true
mod<-glm(mf~area*agegrp, data=dat, family=binomial(link="logit"))
summary(mod)
```
:::


## Interpreting ORs for interaction
Compare the logit for persons aged 40+ living in rainforest areas, $\hat{\beta_0} + \hat{\beta_1} + \hat{\beta_4} + \hat{\beta_7}$,  to that of children age 5-9 living in savannah areas (baseline),  $\hat{\beta_0}$.

The logit difference (log OR) for this comparison is:
$$\hat{\beta_1} + \hat{\beta_4} + \hat{\beta_7}= 0.6030 + 2.3514 + 0.9510 = 3.9054$$ 

*  $\hat{\beta_1}$ (*main effect*): effect of rainforest areas in 0-5 yo
*  $\hat{\beta_4}$ (*main effect*): effect of age 40+ in savannah areas
*  $\hat{\beta_7}$ (*interaction affect*): combined additional effect age 40+ in rainforest areas.

:::

## Summary

* Maximum likelihood estimation provides a general framework for statistical models including generalized linear models (GLM)
* GLMs extend linear regression for use on discrete or categorical outcomes.
* GLMs are specified by the 
    - the family of distributions used to model the outcome 
    - the link function relating the the expected value of the outcome with the linear regression term ($\beta_0+ \beta_1 x_1 + \dots  + \beta_k x_k$).  
* logistic regression models use the binomial distribution and the logit link function

## Summary

* The expected value of a dichotomous outcome $Y \in {0,1}$ is the probability that $Y=1$ (probability of event). 
* Logistic regression models the conditional probability of a dichotomous outcome given values of the independent variables.
* For given values, $x_1, x_2,\dots ,x_k$, of the independent variables we obtain: 
  - the log odds as $\beta_0+ \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k$ (logit)
  - the odds as $e^{\beta_0+ \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k}$
  - the probability as $\frac{e^{\beta_0+ \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k}}{1+e^{\beta_0+ \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k}}$

## Summary
* The slope coefficient of an independent variable represents the log OR for the outcome associated with a unit change in that variable while keeping the levels or other variables fixed. 
* For a dichotomous exposure variable $e^{\beta}$ represents the OR comparing the exposed  (coded 1) to the unexposed group (0). 
* Without interaction terms, the model assumes homogeneity of ORs over the levels of other variables. 
* We can allow ORs to vary over the levels of another variable (effect modification) by including interaction terms.







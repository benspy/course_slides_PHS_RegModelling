---
title: "Model building"
author: "Ben Spycher"
format: ctupres-revealjs
# footer: |
#         CTU Bern theme for [Quarto Presentations](https://quarto.org/docs/presentations/revealjs/index.html).
#         Code available on [GitHub](https://github.com/aghaynes/CTUquarto).
chalkboard: false
self-contained: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(ggplot2)
library(knitr)
library(RColorBrewer)
```

## Program overview

| Day | Time   | Topic                                                  |
|-----|--------|--------------------------------------------------------|
| Mon | AM     | Simple linear regression                               |
|     | PM     | Multiple linear regression                             |
| Tue | AM     | Introduction to logistic regression                    |
|     | **PM** | **Model building considerations and strategies**       |
| Wed | AM     | Models for stratified designs and categorical outcomes |
|     | PM     | Exercises, QA, wrap-up                                 |

## This afternoon's topics

|                                                                               |
|------------------------------------------------------------------------|
| **Model building considerations and strategies**
|	Testing (Wald and likelihood ratio tests)
|	Assessing linearity of association
|	Purposeful variable selection
|	Special issues in prediction modelling (calibration, discrimination, overfitting)


#	Testing in the logistic model

## Purpose of mulivariate models
Take from linear model slide set

## Steps in model building
Model building often includes at least on or multiple (if not all) following steps 

* Variable selection: Which independent variables to include
* Assessing linearity: Is the association of a continuous independent variable x with g(E(y)) linear?
* Modelling non-linear associations: If not, how should we model it? 
* Assessing interaction: Are there interactions between included variables?

Tools needed: Tests and model-selection criteria



## Maximum likelihood estimation

Recall our simple example: Nine of 12 household contacts of a SARS-CoV-2 positive index case also tested positive; the other 3 tested negative.   

Let's model the number of positive tests $Y$ as a binomial distribution with parameters $\pi$ (the unknown probability of a positive test) and $n=12$ (known):

$$P_{n,\pi}(Y=y)=\binom{n}{y}\pi^y(1-\pi)^{n-y}$$

## Maximum likelihood estimation

In ML estimation we estimate parameters by the values that maximise the 'likelihood' of the observed data. 

Formally, we maximize the *likelihood function*, which in our case is

$$\mathcal{L}(\pi|y=9, n=12)=P_{n=12,\pi}(Y=9)=\binom{12}{9} \pi^9(1-\pi)^{3}$$
*Note*: the likelihood function is a function of the parameter of interest given the observed data.


## Maximum likelihood estimation

ML estimator for the parameter $\pi$  of a binomial distribution $\hat{\pi}_{ML}=\frac{k}{n}$
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
L<-function(p){
  choose(12,9)*p^9*(1-p)^(3)
  }
p<-seq(0,1,0.001)
Likelihood<-L(p)
par(cex=1.3, mai=c(1,1,.5,.5))
plot(p,Likelihood, type="l", col="green", xlab=expression(paste(pi)), lwd=2)
abline(v=9/12, lty=3, col="blue", lwd=2)
text(0.42, 0.021, expression(hat(pi)), col="blue")
text(0.60, 0.02, "=9/12=0.75",  col="blue")
```

## Sampling variability
![](pics/sampling_variation.jpg){fig-align="center"}

## Sampling variability of ML estimator

Frequency of k positive tests if the true probability were 0.75 

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
rf<-dbinom(1:12,12, prob=0.75)
par(cex=1.3, mai=c(1,1,.5,.5))
barplot(rf, xlab="k", names.arg=1:12)
```




## Sampling variability of ML estimator

Each $k$ results in a different likelihood function and ML estimator. Dark colors in the graph indicate the most frequent (under repeated sampling) likelihood functions and ML estimates.

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
Lgen<-function(k,n,p){
  choose(n,k)*p^k*(1-p)^(n-k)
}
n<-12
p<-seq(0,1,0.001)
rf<-dbinom(1:n,n, prob=0.75)
rf_max<-max(rf)
k<-1
Likelihood<-Lgen(k,n,p)
mypallete<-brewer.pal(9,"Greens")
par(cex=1.3, mai=c(1,1,.5,.5))
plot(p,Likelihood, type="l", col=mypallete[round(rf[k]*100)], xlab=expression(paste(pi)), lwd=2, ylim=c(0,0.5))
for (k in 1:n) {
  Likelihood<-Lgen(k,n,p)
  lines(p,Likelihood, type="l", col=mypallete[round(rf[k]/rf_max*9)], lwd=2)
  abline(v=k/n, lty=3, col=mypallete[round(rf[k]/rf_max*9)], lwd=2)
}

```



## Sampling variability of ML estimator

Increasing sample size increases precision (here $n=100$)

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
Lgen<-function(k,n,p){
  choose(n,k)*p^k*(1-p)^(n-k)
}
n<-100
p<-seq(0,1,0.001)
rf<-dbinom(1:n,n, prob=0.75)
rf_max<-max(rf)
k<-1
Likelihood<-Lgen(k,n,p)
mypallete<-brewer.pal(9,"Greens")
par(cex=1.3, mai=c(1,1,.5,.5))
plot(p,Likelihood, type="l", col=mypallete[round(rf[k]*100)], xlab=expression(paste(pi)), lwd=2, ylim=c(0,0.12))
for (k in 1:n) {
  Likelihood<-Lgen(k,n,p)
  lines(p,Likelihood, type="l", col=mypallete[round(rf[k]/rf_max*9)], lwd=2)
  abline(v=k/n, lty=3, col=mypallete[round(rf[k]/rf_max*9)], lwd=2)
}

```



## Log-likelihood function
The log of the likelihood function has useful properties. Shown here the log-likelihood function $\ell(\pi)=\mathrm{log}(\mathcal{L}(\pi))$
for $k=75$ and $n=100$:

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
Lgen<-function(k,n,p){
  choose(n,k)*p^k*(1-p)^(n-k)
}
n<-100
k<-75
p<-seq(0,1,0.001)
p0<-0.75
LogLikelihood<-log(Lgen(k,n,p))
approx<--0.5*n/(p0*(1-p0))*(p-p0)^2+max(LogLikelihood)
par(cex=1.3, mai=c(1,1,.5,.5))
plot(p,LogLikelihood, type="l", col="black", xlab=expression(paste(pi)), lwd=2, ylim=c(-50,0))
abline(v=k/n, lty=3, col="blue", lwd=2)
text(k/n-0.03, -48, expression(hat(pi)), col="blue")
```

## Quadratic approximation
In the peak region, the log-likelihood function (in green) is well approximated by a quadratic function (red) sharing the same curvature at the maximum.

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
Lgen<-function(k,n,p){
  choose(n,k)*p^k*(1-p)^(n-k)
}
n<-100
k<-75
p<-seq(0,1,0.001)
p0<-0.75
LogLikelihood<-log(Lgen(k,n,p))
approx<--0.5*n/(p0*(1-p0))*(p-p0)^2+max(LogLikelihood)
par(cex=1.3, mai=c(1,1,.5,.5))
plot(p,LogLikelihood, type="l", col="black", xlab=expression(paste(pi)), lwd=2, ylim=c(-50,0))
lines(p,approx, type="l", col="red", lwd=2)
abline(v=k/n, lty=3, col="blue", lwd=2)
text(k/n-0.03, -48, expression(hat(pi)), col="blue")
text(0, -5, "Log likelihood", col="black", pos=4)
text(0, -10, "Quadratic approximation", col="red", pos=4)

```


## Fisher Information
The curvature of the log-likelihood function serves as a measure of "information" contained in the data about the parameter of interest. 

This measure is called *Fisher information*:  

$\mathcal{I}=-\mathrm{E}[\ell^{''}_i(\pi_0)]$

where $\mathrm{E}$ denotes the expectation and $\ell^{''}_i(\pi_0)$ is the second derivative of the log-likelihood function (for a single observation) evaluated at the true parameter $\pi_0$.

More generally, when there are $p$ parameters, $\mathcal{I}$ is a $p\times p$ matrix (*Fisher information matrix*).



## Standard error of the ML estimator
The $SE$  of the ML estimator is inversely related to the curvature of log-likelihood function at its maximum. 

Specifically, for large $n$, the $SE$ of the parameter is well approximated by

$$
SE_{\hat\pi_{ML}}\approx \frac{1}{\sqrt{n \cdot \mathcal{I}}}
$$

Importantly, the $SE$ decreases inverse proportionally with $\sqrt{n}$ (under certain conditions that are commonly satisfied).

## The ML estimator is consistent and asymptotically normal
Again under certain conditions and assuming that the data arose from the hypothesized distribution, the following holds: 

As $n$ increases, the ML estimator $\hat{\pi}_{ML}$ coverges to the true value $\pi_0$.

More specifically

$$
\sqrt{n}(\hat{\pi}_{ML}-\pi_0) \rightarrow \mathcal{N}(0,\frac{1}{\sqrt{\mathcal{I}}})
$$
This means that, for reasonably large sample sizes, the sampling distribution of $\hat{\pi}_{ML}$ is approximately normal with mean $\pi_0$ and variance $SE_{\hat\pi_{ML}}^2$.


## Testing procedures
Coming back to our example, let's assume we want to test the hypothesis:

$$H_0:\pi_0 =0.6$$
We will get to know two common testing procedures: 

* Wald tests: based on the quadratic approximation of the log-likelihood
* Likelihood ratio (LR) tests: based on the actual log-likelihood function

## Testing procedures

The LR-test statistic is the difference between the log-likelihood (scaled by factor 2) evaluated at its maximum and at the hypothesized parameter: $\lambda_{ML}=2(\ell(\hat{\pi}_{ML})-\ell(\pi_0))$. 

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
Lgen<-function(k,n,p){
  choose(n,k)*p^k*(1-p)^(n-k)
}
n<-100
k<-75
p<-seq(0,1,0.001)
p0<-0.75
ph0<-0.6
LogLikelihood<-log(Lgen(k,n,p))
lmax<-max(LogLikelihood)
approx<--0.5*n/(p0*(1-p0))*(p-p0)^2+lmax
par(cex=1.3, mai=c(1,1,.5,.5))
plot(p,2*LogLikelihood, type="l", col="black", xlab=expression(pi), ylab="2 x log-likelihood", lwd=2, ylim=c(-20,-3))
lines(p,2*approx, type="l", col="red", lwd=2)
abline(v=k/n, lty=3, col="blue", lwd=2)
text(k/n-0.03, -19, expression(hat(pi)), col="blue")
abline(h=2*lmax, lty=3, col="blue", lwd=2)
abline(v=0.6, lty=3, col="blue", lwd=2)
text(0.6+0.03, -19.6, expression(pi[0]), col="blue")
lines(ph0-c(0.004,0.004),c(2*lmax,2*log(Lgen(k,n,0.6))), col="black", lwd=4)
#lines(ph0+c(0.004,0.004),c(2*lmax,-n/(p0*(1-p0))*(ph0-p0)^2+2*lmax), col="red", lwd=4)
text(ph0-0.06, -7, expression(lambda[LR]), col="black")
#text(ph0+0.04, -6.8, expression(z^2), col="red")
text(0, -10, "Log likelihood", col="black", pos=4)
text(0, -12, "Quadratic approximation", col="red", pos=4)

```



## Testing procedures
The same evaluations based on the quadratic approximation yield the Wald test statistic $z^2$. Under $H_0$ and repeated sampling, both $z^2$ and $\lambda_{ML}$ are approximately (for large $n$) $\chi^2$-distributed with 1 df. 


```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
Lgen<-function(k,n,p){
  choose(n,k)*p^k*(1-p)^(n-k)
}
n<-100
k<-75
p<-seq(0,1,0.001)
p0<-0.75
ph0<-0.6
LogLikelihood<-log(Lgen(k,n,p))
lmax<-max(LogLikelihood)
approx<--0.5*n/(p0*(1-p0))*(p-p0)^2+lmax
par(cex=1.3, mai=c(1,1,.5,.5))
plot(p,2*LogLikelihood, type="l", col="black", xlab=expression(pi), ylab="2 x log-likelihood", lwd=2, ylim=c(-20,-3))
lines(p,2*approx, type="l", col="red", lwd=2)
abline(v=k/n, lty=3, col="blue", lwd=2)
text(k/n-0.03, -19, expression(hat(pi)), col="blue")
abline(h=2*lmax, lty=3, col="blue", lwd=2)
abline(v=0.6, lty=3, col="blue", lwd=2)
text(0.6+0.03, -19.6, expression(pi[0]), col="blue")
lines(ph0-c(0.004,0.004),c(2*lmax,2*log(Lgen(k,n,0.6))), col="black", lwd=4)
lines(ph0+c(0.004,0.004),c(2*lmax,-n/(p0*(1-p0))*(ph0-p0)^2+2*lmax), col="red", lwd=4)
text(ph0-0.06, -7, expression(lambda[LR]), col="black")
text(ph0+0.04, -6.8, expression(z^2), col="red")
```

## Chi-squared distribution
Consider $Z_1, Z_2, \dots, Z_k$ independently sampled from the standard normal distribution. The sum of squares $Q=\sum_{i=1}^k Z_i^2$ is then chi-squared distributed with $k$ degrees of freedom. 

Plot of the chi-square distribution for values of $k = 1, 2,\dots, 9$

![Source:<http://commons.wikimedia.org/wiki/User:Geek3>](pics/Chi-square.png){width=80%}


## Wald test
In the case of a single parameter, the square root of the Wald statistic becomes our familiar z-statistic 
$$
z=  \frac{\hat{\pi}_{ML}-\pi_0}{\hat{SE}}
$$
and can be compared with the standard normal distribution.

In our example, $\hat{SE}=\sqrt{\frac{\hat{\pi}_{ML}(1-\hat{\pi}_{ML})}{n}}$ and hence $z=3.4641$ resulting in a p-value of 0.000532.

We can safely reject $H_0$.





## LR-test - single parameter case
We refer to the ratio of the likelihood function for two given parameter values $\pi_1$ and $\pi_2$ as the likelihood ratio $LR=\frac{\mathcal{L}(\pi_0)}{\mathcal{L}(\hat{\pi}_{ML})}$.

The LR-test statistic is 2 times the negative LR with comparing the restricted parameter value under $H_0$ with ML estimate:   
$$
\lambda_{LR}=  -2(\ell(\pi_0) - \ell(\hat{\pi}_{ML}))=-2\cdot \mathrm{log}\left( \frac{\mathcal{L}(\pi_0)}{\mathcal{L}(\hat{\pi}_{ML})}\right)=-2\cdot \mathrm{log}\left(LR\right)
$$
Under $H_0$, $\lambda_{LR}$ follows a $\chi^2$ distribution with 1 df. 

Here $\ell(\pi_0)=-7.3738$ and $\ell(\hat{\pi}_{ML})=-2.3881$  and hence $\lambda_{LR}=9.9714$ resulting in a p-value of 0.00159. Again, we can safely reject $H_0$.


## LR-test - general case
When there are multiple parameters, we can test multiple restrictions. 

Example: In the logistic regression model 
$$\text{logit}(\pi(x_1, x_2,\dots,x_k))=\beta_0 + \beta_1 x_1 +\beta_2 x_2 +\dots +\beta_k x_k$$
we may wish to test $H_0: \beta_1=\beta_2=\beta_3=0$.

Under $H_0$, LR statistic $\lambda_{LR}=  -2(\ell(\hat{\boldsymbol\beta}_{H_0}) - \ell(\hat{\boldsymbol\beta}))$
follows a $\chi^2$ distribution with 3 df (3 restrictions). 

Here $\hat{\boldsymbol\beta}$ stands for the vector of fitted parameters of the full model $\hat{\boldsymbol\beta}_{H_0}$ for those of the restricted model under $H_0$ (i.e dropping the first 3 variables).



## Tests in standard glm output
Output using `summary` command: 

```{r}
#| echo: false
#| eval: true
dat<-read.csv("data/onch1302.csv", sep=";")

dat<- dat |> select(area, agegrp, mf) |> mutate(mf = factor(mf, levels=c(0, 1), labels=c("No", "Yes")),
                    agegrp = factor(agegrp, levels=c(0, 1, 2, 3), labels=c("5–9", "10–19", "20–39", "40+")), 
                    area = factor(area, levels=c(0, 1), labels=c("Savannah", "Rainforest")))

dat$area<-relevel(dat$area, "Savannah")
```

```{r}
#| echo: true
#| eval: true
mod<-glm(mf~area + agegrp, data=dat, family=binomial(link="logit"))
summary(mod)
```
## Tests in standard glm output
We can also use the `tidy` and `glance` functions from the `broom` package to view the glm object
```{r}
#| echo: true
#| eval: true
tidy(mod)
glance(mod)
```

## Tests in standard glm output
The z-value and p-values reported for each estimate refer to the Wald test for the null-hypothesis that the corresponding $\beta$-parameter is zero:  $H_0: \beta_i=\beta_{H_0}= 0$. 

Example: The z-statistic for the `areaRainforest` parameter is: 

$$z= \frac{\hat{\beta}_1-\beta_{H_0}}{\hat{SE}(\hat{\beta}_1)}=\frac{1.1260-0}{0.1376}=8.181$$
The area under the standard normal curve beyond $|z|$ gives the two-sided p-value as reported
```{r}
#| echo: true
#| eval: true
output<-tidy(mod)
2*pnorm(-abs(output$estimate[2]/output$std.error[2]))
```


## Tests in standard glm output
Interpretation of null-hypothesis   $H_0: \beta_i= 0$: 

- For intercept,  $\beta_0= 0$ means that the baseline odds is 1.
- For a slope coefficient,  $\beta_i= 0$ means that the corresponding OR is 1.

We can safely reject the hypothesis that the odds of microfilarial infection is the same in rainforest areas as in savannah areas.


## Deviance 

The deviance of a model is given by: 

$$D=-2(\ell(\hat{\boldsymbol\beta}) - \ell(\hat{\boldsymbol\beta}_{S}))$$

Where $\hat{\boldsymbol\beta}$ is the vector fitted parameters of the model in question, and $\hat{\boldsymbol\beta}_{S}$ that of the saturated model. 

The *saturated model* is one that has a parameter for every observation and thus fits the data exactly. 

The deviance is a generalization of the residual sum of squares (RSS) in linear regression. For the linear model, $D=\text{RSS}$.

## global LR-test

The standard output reports the deviance of the fitted model ("residual deviance", denoted here as $D$) and the null model ("null deviance", $D_{H_0}$), which includes the intercept only. 

We can use these to run an LR-test for the null-hypothesis: 

$$H_0=\beta_1 = \beta_2 =\dots =\beta_k =0$$
The test statistic is simply the difference in deviance: 

 $$\lambda_{LR}= -2(\ell(\hat{\boldsymbol\beta}_{H_0}) - \ell(\hat{\boldsymbol\beta}))=D_{H_0}-D $$

and can be compared to  $\chi^2$ distribution with $k$ df.


## global LR-test
Thus we can directly compute the global LR-test from the output data: 


```{r}
#| echo: true
#| eval: true
out<-unlist(glance(mod))
out
(lambda <- out["null.deviance"]-out["deviance"])
(df <- out["df.null"]-out["df.residual"])

```

## global LR-test
In our example, $D = 1384.8$ and $D_0=1714.1$. Thus the test statistic is $\lambda_{LR}=1714.1-1384.8=329.3$. 

We can use the `anova` command to run the test: 


```{r}
#| echo: true
#| eval: true
mod<-glm(mf~area + agegrp, data=dat, family=binomial(link="logit"))
mod_0<-glm(mf~1, data=dat, family=binomial(link="logit"))
lrt <- anova(mod_0, mod, test="LRT")
lrt
# Exact p-value
lrt["Pr(>Chi)"]

```

## LR-test vs. Walt test
* LR-test is generally recommended as it is based on the actual likelihood ratio
* The LR-test plays the same role in GLMs as the F-test in linear models
* (Partial) LR-test usually require fitting the model twice (unrestricted and restricted model)
* The Wald test avoids this by using a quadratic approximation of the likelihood function
* For this reason, the Wald tests for single parameters are shown in standard regression output 

## LR test for interaction
Recall our model for interaction 

```{r}
#| echo: true
#| eval: true
mod_int<-glm(mf~area*agegrp, data=dat, family=binomial(link="logit"))
summary(mod_int)
```

## LR test for interaction
We can use the LR-test to test whether their is interaction. 

The null-hypothesis statest that the $\beta$-parameters of all 3 interaction terms are 0. 

```{r}
#| echo: true
#| eval: true
anova(mod, mod_int, test="LRT")
```

We find no evidence for interaction between area and age group (P = 0.1531)

## LR test in practice 
Run the LR test using deviance with anova function

## Testing linearity of association
General problem (show graph) coronary heart disease

## Testing linearity of association
Write down procedure steps: 

- Create an categorical variable (factor) representing quintiles
- Create a "continuous" variable representing means within quintiles
- LR test of 2 (included as continuous) nested with 1 

## Testing linearity of association
Show example: calculations in R

## Testing linearity of association
Show example: graphically

## Modelling non-linear associations
Simple method variable transformations: 

- log or some power
- Include x^2 or (x^3 rarely needed)


## Modelling non-linear associations
Show example graphically 

## Modelling non-linear associations
More advanced methods (not for this course): 

- fractional polynomials 
- splines



## Variable selection
Depends on the purpose

- Single exposure: selection of confounding factors, causal inference DAGs
- Prediction modelling: Selection should maximise predictive performence (in an external dataset)
- More general risk factor modelling: Multiple candidate risk factors / predictors; which ones are important (not implying causation)

## Model selection criterion
Also for non-nested comparisons

general formula

## Overfitting 
Introduce general problem, come back later when focusing on prediction

Optimism, shrinkage

## Overfitting 
Describe the general problem

Model fit always increases 

## Overfitting 
Infinite monkey theorem

Model fit always increases 



## AIC
more liberal (lower penalization), favors more parameters


## BIC
more conservative (greater penalization), consistent (chooses the right model as n -> inf)

often preferred

## Variable selection
Different approaches: 

* Selection based on prior knowledge
* Data driven: 
  - Automated stepwise regression
  - "Purposeful selection"
  - Penalized regression (Lasso, Ridge, Elastic net)

## Stepwise regression
General approach

- Backward 
- Forward 
- Alternative

## Stepwise regression in R
Based on AIC/BIC

## Caerphilly dataset 
Introduce caerphily dataset

## Stepwise forward regression
run for caerphilly

## Caerphilly dataset 
stepwise for caerphilly

## Problems of stepwise regression
Generally not recommended

Criticisms by Harrell (ref)

Also investigator (partial) knowledge ignored, doesn't consider non-linear functional forms, interactions


##	Purposeful variable selection
As a viable approach for n>>p

Explain steps


##	Purposeful variable selection
As a viable approach for n>>p

Explain steps


##	Purposeful variable selection
As a viable approach for n>>p

Explain steps


## Prediction modeling
Special issues: 

- Often large number of candidate predictors -> risk of overfitting
- Assessing model performance
  - Discrimination / 

## Further reading: 
[García-Portugués E, Notes for Predictive Modeling](https://bookdown.org/egarpor/PM-UC3M/)


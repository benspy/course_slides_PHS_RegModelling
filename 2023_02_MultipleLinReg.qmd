---
title: "Multiple linear regression"
author: "Ben Spycher"
format: ctupres-revealjs
# footer: |
#         CTU Bern theme for [Quarto Presentations](https://quarto.org/docs/presentations/revealjs/index.html).
#         Code available on [GitHub](https://github.com/aghaynes/CTUquarto).
chalkboard: false
self-contained: true
---
  
```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(knitr)
library(latex2exp)
library(fastDummies)
library(scatterplot3d)
library(car)
```

```{r functions}
#| echo: false
#| eval: true
# Summary function that allows selection of which coefficients to include 
# in the coefficient table
# Works with summary.lm and summary.plm objects
my.summary = function(x, digits=3) {

  # Print a few summary elements that are common to both lm and plm model summary objects
  #cat("Call\n")
  #print(x$call)
  #cat("\nResiduals\n")
  #print(summary(x$residuals))
  #cat("\n")
  print(coef(x))

  # Print elements unique to lm model summary objects
  if("summary.lm" %in% class(x)) {
    cat("\nResidual standard error:", round(x$sigma,3), "on", x$df[2], "degrees of freedom")
    cat(paste(c("\nF-statistic:", " on"," and"), round(x$fstatistic,2), collapse=""),
        "DF, p-value:",
        format.pval(pf(x$fstatistic[1L], x$fstatistic[2L], x$fstatistic[3L], 
                       lower.tail = FALSE), digits=digits))
    cat("\nR squared", round(x$r.squared,3), " adj. R squared", round(x$adj.r.squared,3))

  # Print elements unique to plm model summary objects  
  } else if ("summary.plm" %in% class(x)) {
    cat(paste("\nResidual Sum of Squares: ", signif(deviance(x), 
                                                  digits), "\n", sep = ""))
    fstat <- x$fstatistic
    if (names(fstat$statistic) == "F") {
      cat(paste("F-statistic: ", signif(fstat$statistic), " on ", 
                fstat$parameter["df1"], " and ", fstat$parameter["df2"], 
                " DF, p-value: ", format.pval(fstat$p.value, digits = digits), 
                "\n", sep = ""))
    }
    else {
      cat(paste("Chisq: ", signif(fstat$statistic), " on ", 
                fstat$parameter, " DF, p-value: ", format.pval(fstat$p.value, 
                                                               digits = digits), "\n", sep = ""))
    }
  }
}

```


## Program overview
|Day | Time | Topic
|--- |----- | -------- 
  |Mon| AM | Simple linear regression
  |   | **PM** | **Multiple linear regression**
|Tue| AM | Introduction to logistic regression
|   | PM | Model building considerations and strategies
|Wed| AM | Models for stratified designs and categorical outcomes
|   | PM | Exercises, QA, wrap-up


## This afternoon's topics
|   
|-------------------- 
| **Multiple linear regression**
|	Including multiple independent variables 
|	Model selection ($F$-test, adjusted $R^2$)
|	Residual analysis (Residual plots, leverage, QQ-plot)
|	Multicollinearity (variance inflation factor)



## Multiple linear regression
Generally, with multiple independent variables, $x_1, x_2, \dots, x_k$, we write the linear model as:

$$Y_i= \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} +\dots + \beta_k x_{k,i}+ \varepsilon_i$$

Where the  $\varepsilon_i$ are independently (_i. independence_), normally distributed (_ii. normality_) with mean 0 (_iii. zero mean_) and constant variance $\sigma^2$ (_iv. homoscedasticity_).


## Multiple linear regression
The conditional mean of  $Y_i$ given specific values of the independent variables $x_1, x_2, \dots, x_k$ is thus: 

$$\mu_{y|x_1, x_2,\dots,x_k}=\beta_0 + \beta_1 x_1 +\beta_2 x_2 +\dots +\beta_k x_k$$
We obtain the $k+1$ $\beta$-parameters by least squares estimation. The residual sum of squares have $n-k-1$ degrees of freedom. 


## Multiple linear regression
With 2 independent variables, the least squares solution spans a plane in 3-dimensional space

```{r}
#| echo: false
#| eval: true

data(iris)
# Data, linear regression with two explanatory variables
wh <- iris$Species != "setosa"
x  <- iris$Sepal.Width[wh]
y  <- iris$Sepal.Length[wh]
z  <- iris$Petal.Width[wh]
df <- data.frame(x, y, z)
LM <- lm(y ~ x + z, df)

# scatterplot
s3d <- scatterplot3d(x, z, y, pch = 19, type = "p", color = "darkgrey",
                     main = "Regression Plane", grid = TRUE, box = FALSE,  
                     mar = c(2.5, 2.5, 2, 1.5), angle = 55,
                     xlab="x1", ylab="x2")

# compute locations of segments
orig     <- s3d$xyz.convert(x, z, y)
plane    <- s3d$xyz.convert(x, z, fitted(LM))
i.negpos <- 1 + (resid(LM) > 0) # which residuals are above the plane?

# draw residual distances to regression plane
segments(orig$x, orig$y, plane$x, plane$y, col = "red", lty = c(2, 1)[i.negpos], 
         lwd = 1.5)

# draw the regression plane
s3d$plane3d(LM, draw_polygon = TRUE, draw_lines = TRUE, 
            polygon_args = list(col = rgb(0.8, 0.8, 0.8, 0.8)))

# redraw positive residuals and segments above the plane
wh <- resid(LM) > 0
segments(orig$x[wh], orig$y[wh], plane$x[wh], plane$y[wh], col = "red", lty = 1, lwd = 1.5)
s3d$points3d(x[wh], z[wh], y[wh], pch = 19)


```


## The model in matrix notation
$$\mathbf {y} =\mathbf {X} {\boldsymbol {\beta }}+{\boldsymbol {\varepsilon }}$$
where 
$\mathbf{y} =\begin{bmatrix}y_{1}\\y_{2}\\\vdots \\y_{n}\end{bmatrix},\quad \mathbf{X} ={\begin{bmatrix}1&x_{11}&\cdots &x_{1p}\\1&x_{21}&\cdots &x_{2p}\\\vdots &\vdots &\ddots &\vdots \\1&x_{n1}&\cdots &x_{np}\end{bmatrix}},\quad {\boldsymbol{\beta }}={\begin{bmatrix}\beta _{0}\\\beta _{1}\\\beta _{2}\\\vdots \\\beta _{p}\end{bmatrix}},\quad {\boldsymbol {\varepsilon }}={\begin{bmatrix}\varepsilon _{1}\\\varepsilon _{2}\\\vdots \\\varepsilon _{n}\end{bmatrix}}.$

## The model in matrix notation
Parameter estimates:

$\boldsymbol{\hat{\beta }}=\left(\mathbf {X} ^{\mathsf {T}}\mathbf {X} \right)^{-1}\mathbf {X} ^{\mathsf {T}}\mathbf{y}$

Estimated conditional mean of independent variable:

$\mathbf{\hat{y}}=\mathbf {X}\left(\mathbf {X} ^{\mathsf {T}}\mathbf {X} \right)^{-1}\mathbf {X} ^{\mathsf{T}}\mathbf {y} =\mathbf {H} \mathbf {y}$

Here $\mathbf{H}=\mathbf {X}\left(\mathbf {X} ^{\mathsf {T}}\mathbf {X} \right)^{-1}\mathbf{X} ^{\mathsf{T}}$ is the so-called _hat matrix_ (needed later on to extract leverages).

## What is linear about the model?
The model is linear in the $\beta$-parameters, not in the $x$-variables!


Through variable transformations we can model non-linear associations. e.g.:

$$Y= \beta_0 + \beta_1 x_{1} + \beta_2 x_{1}^2 + \varepsilon$$
becomes 
$$Y= \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \varepsilon$$
with $x_{2}=x_{1}^2$


## Example - quadratic association
True model $E(Y|x)= 1 + 2 x_{1} + 0.8x_{1}^2$
```{r}
#| echo: false
#| eval: true
x<-1:100/25
x_sq<-x^2
Y<-1+2*x+0.8*x_sq+rnorm(0,2, n=100)
plot(x, Y)
reg<-lm(Y ~ x + x_sq)
summary(reg)$coeff
lines(x,fitted.values(reg), col="blue", lwd=1.5)

```


## Modelling objectives
Common reasons for including multiple independent variables are: 

* Risk factor modelling in analytic studies
    + Adjusting for confounding 
    + Mutual adjustment with multiple risk factors
* Prediction modelling 
* Estimating non-linear associations


## Modelling lung function
Recall our regresion of $FEV_1$ on age.



## Peru lung function dataset

Peru lung function data (*perulung_ems.csv*): Lung function data und 636 children living in a deprived suburb of Lima, Peru. 

```{r}
#| echo: false
#| eval: true
library(tidyverse)
data <- read_csv("data/perulung_ems.csv")
data<- data |> mutate(id=as.integer(id), 
                sex = factor(sex, levels=c(0,1), labels=c("f","m")), 
                respsymptoms=factor(respsymptoms, levels=c(0,1), labels=c("no","yes")), 
                asthma_hist=factor(asthma_hist, levels=c("never", "previous asthma", "current asthma"), labels=c("never", "previous asthma", "current asthma")))
```



```{r}
#| echo: false
#| eval: true
my.summary(summary(lm(fev1~age , data=data)))
```


## Including weight
Let's first look at the scatterplots
```{r}
#| echo: true
#| eval: true
#| fig-width: 4
#| fig-height: 4
#| fig-align: "center"
pairs(data[,c("fev1", "age", "height")])
```

## Including weight
Include body height as additional dependent variable: 

$$FEV_{1i}= \beta_0 + \beta_1\cdot \text{age}_i + \beta_2 \cdot \text{height}_i + \varepsilon_i$$

```{r}
#| echo: false
#| eval: true
my.summary(summary(lm(fev1~age + height, data=data)))
```

Observations: 

* The slope for 'age' is mow much smaller (0.090 l/Jahr)  
* $R^2$ has increaset to 0.436.  

## Interpretation of parameters
Age and height are correlated, part of the increase with age can be attrubuted to height.  

According to the current model: 

* Mean $FEV_1$ increases by $\hat{\beta}_1=$ 90 ml per year increase in age among children of the same height (adjusted for height).
* Mean $FEV_1$ increases by $\hat{\beta}_2=$ 25 ml per cm increase in height among children of the same age (adjusted for age).




## F-Test in the output
The F-test in the tests the null hypothesis that all slope parameters are simultaneously zero. 

In our case: 

$$H_0: \beta_1=\beta_2=0$$
With a  P-value $<2\cdot10^{-16}$ the hypothesis can be clearly rejected.


## ..including the variable sex

```{r}
#| echo: false
#| eval: true
my.summary(summary(lm(fev1~age + height + sex, data=data)))
```
 
Observations: 

* The slope parameters for age and height haven't changed much.   
* $R^2$ has slightly increased from 0.436 to 0.475.  
* The degrees of freedom is reduced by 1.   



## Adjusted $R^2$
$R^2$ can only increase when a new independent variable is added, regardless of whether the variable is truly informative (associated in the population) or not.

The adjusted $R^2$ takes into account the loss of degrees of freedom from adding a variable: 

$$\text{adj. }R^2= 1-\frac{RSS/(n-p-1)}{TSS/(n-1)}$$

The term $RSS/(n-p-1)$ tends to increase (and $\text{adj. }R^2$ decrease) when non-informative variables are added. 


## ..including asthma history

```{r}
#| echo: false
#| eval: true
my.summary(summary(lm(fev1~age + height + sex + asthma_hist, data=data)))
```
 
Observations: 

* Little change in the slopes of previously included variables.
* Very slight increase in $\text{adj. } R^2$ from 0.473 to 0.479.  
* Degrees of freedom is reduced by 2.   



## ..including current symptoms

```{r}
#| echo: false
#| eval: true
my.summary(summary(lm(fev1~age + height + sex + asthma_hist + respsymptoms, data=data)))
```
 
Observations: 

* Model fit $\text{adj. } R^2$ has not improved.  
* There is, however, strong evidence for a difference between those with and without current symptoms (P<0.001)

## Global vs. partial F-test
The partial $F$-test tests the null-hypothesis that the $\beta$-parameters of the multiple selected variables are zero. 

1. Fit the full model and store the residual stum of squares ($RSS$)
2. Fit the restricted model without the variables ($RSS_0$)

The test statistic $F=\frac{(\text{RSS}_0-\text{RSS})/k}{\text{RSS}/(n-p-1)}$ follows an $F$-distrubion with $k$ and $n-p-1$ df. Here, $k$ is the number of restricted $\beta$-parameters (omitted in 2.) 

- _Global $F$-Test_: $k=p$ (standard output)

- _Partial $F$-Test_: $1\leq k<p$ (Equivalent to $t$-test when $k=1$ )

## Partial F-test in practice
Let's test for differences by asthma history ($k=2$ restrictions)
```{r}
#| echo: true
#| eval: true
mod_full <- lm(fev1~age + height + sex + asthma_hist + respsymptoms, data=data)
mod_restricted <- lm(fev1~age + height + sex + respsymptoms, data=data)
anova(mod_full, mod_restricted)
```
* Evidence for differences by asthma history, even when adjusted for other variables including symptoms (P=0.01)



## Partial F-test in practice
Let's test the combined contribution of asthma history and symptoms ($k=3$ restrictions). 
```{r}
#| echo: true
#| eval: true
mod_full <- lm(fev1~age + height + sex + asthma_hist + respsymptoms, data=data)
mod_restricted <- lm(fev1~age + height + sex, data=data)
anova(mod_full, mod_restricted)
```
* Strong evidence for a combined contribution after adjusting for age, height, sex (P<0.0001)
 
## Residual analysis
A closer look at residuals allows us to detect strong violations of model assumptions or the undue influence of single observations. 

The command `plot(`_model name_`)` will show four residual plots: 

1. Residuals vs. fitted values
2. Q-Q plot
3. Scale location plot
4. Cook's distance plot
5. Residuals vs. Levarage
6. Cook's dist vs Lev./(1-Lev.)

We will focus on 1,2,3,5 which are shown by default.

## Plot 1: Residuals vs. fitted values
Checking the _zero mean_ assumption: The residuals should have  mean zero (approximately) over the entire range of fitted values
```{r}
#| echo: true
#| eval: true
mod_final <- lm(fev1~age + height + sex + asthma_hist + respsymptoms, data=data)
plot(mod_final, which=1)
```
## Standardizing residuals
Residuals are estimates (of errors) and thus subject to sampling variation. 

Their variance is not constant: $\text{var}(\hat{\varepsilon_i})=\sigma^2(1-h_{ii})$, where $h_{ii}$ is the $i$-th diagonal element of the hat matrix. 

The _standardized_ (or _studentized_) residuals

$$t_i=\frac{\hat{\varepsilon_i}}{\hat{\sigma}\sqrt{(1-h_{ii}}}$$
will have constant variance of 1. 

## Plot 2: Q-Q plot
Checking the _normality_ assumption: The standardized residuals should follow (approximately) a standard normal distribution. The points in the Q-Q plot should align closely with the diagonal. 
```{r}
#| echo: true
#| eval: true
plot(mod_final, which=2)
```

## Misconceptions about normality
1. The outcome must be normally distribution to run linear regression (or a t-test, or ANOVA)

FALSE: The normality assumption applies only to the residuals. You need to first fit the model. 

2. Violation of normality invalidates the model

FALSE: The model is robust against violations of normality if sample size (more precisely the residual df) is reasonably large and there are no points with large leverage (see below)

## Plot 3: Scale-location
Checking the _homoscedasticity_ assumption: The square root of the standardized residuals should have constant mean over the range of fitted values. 
```{r}
#| echo: true
#| eval: true
plot(mod_final, which=3)
```


## Leverage
The leverage of observation $i$ is given by  $h_{ii}$, the $i$-th diagonal element of the hat matrix. 

$h_{ii}$ measures how far away the values of the independent variables if the observation are from those of other observations. 
 
In simple linear regression:  $h_{ii}= \frac{1}{n} + \frac{(x_i-\bar{x})^2}{\sum_j(x_j-\bar{x})^2}$

Observations with high leverage can have a strong influence on fitted model. 

## Influence
An observation with high leverage will influence the regression if it does not fit the general trend of the other observations
```{r}
#| echo: false
#| eval: true

set.seed(1234)
x <- c(rnorm(20, mean=7, sd=2), 18)
y <- 2+x+c(rnorm(20), 0.1)
par(mfcol = c(1, 2))
plot(x,y,xlim=c(0,20), ylim=c(0,22), main="Low influence")
abline(lm(y~x), col="blue")
points(x[21],y[21], col="red", pch=16)
y[21] <- 10
plot(x,y,xlim=c(0,20), ylim=c(0,22), main="High influence")
abline(lm(y~x), col="blue")
points(x[21],y[21], col="red", pch=16)
```


## Cook's distance
A common measure of influence is _Cook's distance_: 

$$D_i=\frac{\sum_j\left(\hat{y}_j-\hat{y}_{j(i)}\right)}{(p+1)\hat{\sigma}^2}$$
Where the $\hat{y}_j$ and $\hat{y}_{j(i)}$ are the residuals from a model with and without observation $i$ respectively. 

Cook's distance can calculated directly from the standardized residual and leverage of observation $i$: 

$$D_i=\frac{1}{p+1}\cdot t_i^2\cdot\frac{h_{ii}}{1-h_{ii}}$$

## Plot 5: Residuals vs. Levarage
Residuals with high influence, i.e. high leverage and large residual, will lie close to the right top or bottom corners
```{r}
#| echo: true
#| eval: true
plot(mod_final, which=5)
```

## Influence
For our toy example: 
```{r}
#| echo: false
#| eval: true

set.seed(1234)
x <- c(rnorm(20, mean=7, sd=2), 18)
y <- 2+x+c(rnorm(20), 0.1)


par(mfcol = c(2, 2), mai=c(0.7,0.7,0.2, 0.5))
plot(x,y,xlim=c(0,20), ylim=c(0,22), main="Low influence")
mod<-lm(y~x)
abline(mod, col="blue")
points(x[21],y[21], col="red", pch=16)
plot(mod, which=5)
y[21] <- 10
plot(x,y,xlim=c(0,20), ylim=c(0,22), main="High influence")
mod<-lm(y~x)
abline(mod, col="blue")
points(x[21],y[21], col="red", pch=16)
plot(mod, which=5)
````

## Multicollinearity

Occurs when an independent variable can be predicted with high accuracy by a linear combination of the others.

Possible consequences:  

* Corresponding coefficients are estimated with low precision (large $SE$s).  
* Estimated coefficients become sensitive to inclusion of particular variables often changing direction.  
* The effects of correlated variables may counteract each other and become large in opposite directions.  
    
## Example - Davis dataset
Measured vs self-reported height and weight in 88 men. Pearson correlation coefficients: 
```{r}
#| echo: false
#| eval: true
#| fig-width: 4
#| fig-height: 4
#| fig-align: "center"

data(Davis)
dat <- Davis[Davis$sex=="M", c("weight", "repwt", "height", "repht") ]
pairs(dat)
cor(dat[complete.cases(dat),])
```

## Example - Davis dataset
How well does self-reported weight predict measured weight
```{r}
#| echo: false
#| eval: true
my.summary(summary(lm(weight~repwt , data=dat)))
```

## Example - Davis dataset

What if we add measured height
```{r}
#| echo: false
#| eval: true
my.summary(summary(lm(weight~repwt + height, data=dat)))
```

.. and, at the risk redundancy, self-reported height:

```{r}
#| echo: false
#| eval: true

reg<-lm(weight~repwt + height + repht, data=dat)
my.summary(summary(reg))
```


## Variance inflation factor

In multiple linear regression the sampling variance ($SE_{\beta_i}^2$) of the parameter $\beta_i$ of a given variable $x_i$ can be written as:  

$$SE_{\beta_i}^2=\frac{s^2}{(n-1)s_{x_i}^2}\cdot\frac{1}{1-R_i^2}$$  

where $R_i^2$ is the $R^2$ from a linear regression of $x_i$ on all other independent variables.

The term $\frac{1}{1-R_i^2}$ is the _variance inflaction factor_ (VIF).


## Example - Davis dataset
$VIF$s in our final model 

```{r}
#| echo: true
#| eval: true
library(car)
reg<-lm(weight~repwt + height + repht, data=dat)
vif(reg)
```


Note: 

* High $VIF$ values does not imply that $SE$'s will be large
* Large sample sizes, by reducing $SE$, mitigate effects of multicollinearity
* However, $VIF$s are useful for identifying problematic variables


## Dealing with multicollinearity
When adding a new variable look out for: 

* Increases in standard errors 
* Changes in the magnitude/sign of slope parameters

What can you do: 

* Check correlations/$VIF$ to identify problem variables
* Is one of the variables redundant; can it be excluded? (check $R^2$)
* If not, can the variables be combined to single variable


::: {.callout-note}
Multicollinearty negatively affects parameter estimation but not prediction
:::


## Summary


## Summary









